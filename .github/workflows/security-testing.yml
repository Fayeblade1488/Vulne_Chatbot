name: Security Testing Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run daily security scan at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_level:
        description: 'Test Level'
        required: true
        default: 'standard'
        type: choice
        options:
          - quick
          - standard
          - comprehensive
          - paranoid

env:
  PYTHON_VERSION: '3.11'
  NODE_VERSION: '18'
  RESULTS_RETENTION_DAYS: 30

jobs:
  setup:
    name: Setup and Cache Dependencies
    runs-on: ubuntu-latest
    outputs:
      cache-key: ${{ steps.cache-keys.outputs.key }}
    steps:
      - uses: actions/checkout@v3
      
      - name: Generate cache keys
        id: cache-keys
        run: |
          echo "key=deps-${{ hashFiles('**/requirements.txt', '**/package-lock.json') }}" >> $GITHUB_OUTPUT
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Setup Node.js
        uses: actions/setup-node@v3
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: frontend/package-lock.json
      
      - name: Cache dependencies
        uses: actions/cache@v3
        with:
          path: |
            ~/.cache/pip
            ~/node_modules
            ~/.npm
          key: ${{ steps.cache-keys.outputs.key }}

  lint-and-format:
    name: Code Quality Checks
    needs: setup
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install linting tools
        run: |
          pip install flake8 black mypy isort pylint
      
      - name: Run Python linters
        run: |
          flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
          black --check app/ benchmarking/ scripts/
          isort --check-only app/ benchmarking/ scripts/
          mypy app/ benchmarking/ scripts/ --ignore-missing-imports || true
      
      - name: Run frontend linters
        working-directory: frontend
        run: |
          npm ci
          npm run lint || true

  security-scan:
    name: Security Vulnerability Scan
    needs: setup
    runs-on: ubuntu-latest
    permissions:
      security-events: write
    steps:
      - uses: actions/checkout@v3
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install security tools
        run: |
          pip install bandit safety semgrep pip-audit
      
      - name: Run Bandit security scan
        run: |
          bandit -r app/ benchmarking/ scripts/ -f json -o bandit-report.json || true
      
      - name: Run Safety check
        run: |
          safety check --json > safety-report.json || true
      
      - name: Run pip-audit
        run: |
          pip-audit --desc --format json > pip-audit-report.json || true
      
      - name: Run Semgrep
        uses: returntocorp/semgrep-action@v1
        with:
          config: auto
      
      - name: Upload security reports
        uses: actions/upload-artifact@v3
        with:
          name: security-reports
          path: |
            *-report.json
          retention-days: ${{ env.RESULTS_RETENTION_DAYS }}
      
      - name: Check for critical vulnerabilities
        run: |
          python -c "
          import json
          import sys
          
          critical_found = False
          
          # Check bandit report
          try:
              with open('bandit-report.json', 'r') as f:
                  bandit = json.load(f)
                  high_severity = [i for i in bandit.get('results', []) if i.get('issue_severity') == 'HIGH']
                  if high_severity:
                      print(f'Found {len(high_severity)} high severity issues')
                      critical_found = True
          except: pass
          
          # Check safety report
          try:
              with open('safety-report.json', 'r') as f:
                  safety = json.load(f)
                  if safety:
                      print(f'Found {len(safety)} dependency vulnerabilities')
                      critical_found = True
          except: pass
          
          if critical_found:
              print('⚠️  Critical security issues found!')
              sys.exit(1)
          else:
              print('✅ No critical security issues found')
          "

  unit-tests:
    name: Unit Tests
    needs: setup
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ['3.9', '3.10', '3.11']
    steps:
      - uses: actions/checkout@v3
      
      - name: Setup Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}
      
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install pytest pytest-cov pytest-asyncio pytest-mock
      
      - name: Run unit tests
        run: |
          pytest tests/unit/ -v --cov=app --cov=benchmarking \
            --cov-report=xml --cov-report=html --cov-report=term
      
      - name: Upload coverage reports
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml
          flags: unittests
          name: codecov-umbrella

  integration-tests:
    name: Integration Tests
    needs: [lint-and-format, unit-tests]
    runs-on: ubuntu-latest
    services:
      redis:
        image: redis:alpine
        ports:
          - 6379:6379
      postgres:
        image: postgres:14
        env:
          POSTGRES_PASSWORD: postgres
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
    steps:
      - uses: actions/checkout@v3
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install -e benchmarking/
      
      - name: Setup Ollama (for model testing)
        run: |
          curl -fsSL https://ollama.ai/install.sh | sh
          ollama serve &
          
          # Wait for Ollama to be ready with proper health check
          for i in {1..30}; do
            if curl -s http://localhost:11434/api/tags >/dev/null 2>&1; then
              echo "Ollama is ready"
              break
            fi
            echo "Waiting for Ollama to start... ($i/30)"
            sleep 2
          done
          
          # Verify Ollama is responsive before pulling models
          if ! curl -s http://localhost:11434/api/tags >/dev/null 2>&1; then
            echo "Failed to start Ollama service"
            exit 1
          fi
          
          ollama pull mistral:latest
      
      - name: Run integration tests
        env:
          DATABASE_URL: postgresql://postgres:postgres@localhost:5432/test_db
          REDIS_URL: redis://localhost:6379
        run: |
          pytest tests/integration/ -v --tb=short
      
      - name: Test vulnerable endpoints
        run: |
          python app/vulne_chat.py &
          APP_PID=$!
          
          # Wait for app to be ready with health check
          for i in {1..30}; do
            if curl -s http://localhost:7000/health >/dev/null 2>&1; then
              echo "App is ready"
              break
            fi
            echo "Waiting for app to start... ($i/30)"
            sleep 2
          done
          
          # Test health endpoint
          if ! curl -f http://localhost:7000/health; then
            echo "Health check failed"
            kill $APP_PID
            exit 1
          fi
          
          # Verify model is available before testing chat
          echo "Checking if model is available..."
          curl -s http://localhost:11434/api/tags | grep -q "mistral:latest" || {
            echo "Model mistral:latest not available, skipping chat test"
            kill $APP_PID
            exit 1
          }
          
          # Test chat endpoint
          if ! curl -X POST http://localhost:7000/chat \
            -H "Content-Type: application/json" \
            -d '{"message": "test", "model": "mistral:latest"}'; then
            echo "Chat endpoint test failed"
            kill $APP_PID
            exit 1
          fi
          
          echo "All endpoint tests passed"
          kill $APP_PID

  benchmark-tests:
    name: Security Benchmark Tests
    needs: [integration-tests, security-scan]
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || github.event.inputs.test_level == 'comprehensive' || github.event.inputs.test_level == 'paranoid'
    steps:
      - uses: actions/checkout@v3
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install -e benchmarking/
      
      - name: Setup Ollama
        run: |
          curl -fsSL https://ollama.ai/install.sh | sh
          ollama serve &
          
          # Wait for Ollama to be ready
          for i in {1..30}; do
            if curl -s http://localhost:11434/api/tags >/dev/null 2>&1; then
              echo "Ollama is ready"
              break
            fi
            echo "Waiting for Ollama to start... ($i/30)"
            sleep 2
          done
          
          # Verify Ollama is responsive
          if ! curl -s http://localhost:11434/api/tags >/dev/null 2>&1; then
            echo "Failed to start Ollama service"
            exit 1
          fi
          
          ollama pull mistral:latest
          ollama pull llama2:latest
      
      - name: Start vulnerable app
        run: |
          python app/vulne_chat.py &
          
          # Wait for app to be ready
          for i in {1..30}; do
            if curl -s http://localhost:7000/health >/dev/null 2>&1; then
              echo "App is ready for benchmarking"
              break
            fi
            echo "Waiting for app to start... ($i/30)"
            sleep 2
          done
          
          # Verify app is responsive
          if ! curl -s http://localhost:7000/health >/dev/null 2>&1; then
            echo "Failed to start vulnerable app"
            exit 1
          fi
      
      - name: Run Garak benchmarks
        run: |
          python benchmarking/vulne_bench/runners/run_garak.py \
            --target-url http://localhost:7000/chat \
            --model mistral:latest \
            --output-dir results/garak
      
      - name: Run NeMo benchmarks
        run: |
          python benchmarking/vulne_bench/runners/run_nemo.py \
            --target-url http://localhost:7000/chat \
            --model mistral:latest \
            --output-dir results/nemo
      
      - name: Run GuardrailsAI benchmarks
        run: |
          python benchmarking/vulne_bench/runners/run_guardrailsai.py \
            --target-url http://localhost:7000/chat \
            --model mistral:latest \
            --output-dir results/guardrailsai
      
      - name: Generate consolidated report
        run: |
          # Check if report generator exists
          if [ -f "benchmarking/report_generator.py" ]; then
            python benchmarking/report_generator.py \
              --input-dir results \
              --output-file benchmark-report.html
          else
            echo "Report generator not found, skipping consolidated report"
            echo "Individual benchmark results available in results/ directory"
          fi
      
      - name: Upload benchmark results
        uses: actions/upload-artifact@v3
        with:
          name: benchmark-results-${{ github.run_number }}
          path: |
            results/
            benchmark-report.html
          retention-days: ${{ env.RESULTS_RETENTION_DAYS }}

  vulnerability-testing:
    name: Vulnerability Testing
    needs: [integration-tests]
    runs-on: ubuntu-latest
    if: github.event.inputs.test_level == 'paranoid' || github.event_name == 'schedule'
    steps:
      - uses: actions/checkout@v3
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
      
      - name: Start vulnerable app for testing
        run: |
          python app/vulne_chat.py &
          
          # Wait for app to be ready
          for i in {1..30}; do
            if curl -s http://localhost:7000/health >/dev/null 2>&1; then
              echo "App is ready for vulnerability testing"
              break
            fi
            echo "Waiting for app to start... ($i/30)"
            sleep 2
          done
      
      - name: Generate AI-powered probes
        run: |
          python scripts/ai_probe_generator.py
      
      - name: Run custom vulnerability tests
        run: |
          if [ -f "scripts/vulnerability_tester.py" ]; then
            python scripts/vulnerability_tester.py \
              --probes generated_probes.json \
              --target http://localhost:7000 \
              --output vuln-test-results.json
          else
            echo "Custom vulnerability tester not found, skipping"
            echo "Generated probes are still available for manual testing"
          fi
      
      - name: Upload vulnerability results
        uses: actions/upload-artifact@v3
        with:
          name: vulnerability-results-${{ github.run_number }}
          path: |
            generated_probes.json
            vuln-test-results.json
          retention-days: ${{ env.RESULTS_RETENTION_DAYS }}

  docker-build:
    name: Docker Build and Scan
    needs: [lint-and-format]
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2
      
      - name: Build Docker image
        uses: docker/build-push-action@v4
        with:
          context: .
          push: false
          tags: vulne-chatbot:${{ github.sha }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          outputs: type=docker,dest=/tmp/image.tar
      
      - name: Run Trivy vulnerability scanner
        uses: aquasecurity/trivy-action@master
        with:
          input: /tmp/image.tar
          format: 'sarif'
          output: 'trivy-results.sarif'
      
      - name: Upload Trivy results to GitHub Security
        uses: github/codeql-action/upload-sarif@v2
        with:
          sarif_file: 'trivy-results.sarif'

  notification:
    name: Send Notifications
    needs: [benchmark-tests, vulnerability-testing, docker-build]
    runs-on: ubuntu-latest
    if: always()
    steps:
      - name: Determine status
        id: status
        run: |
          if [[ "${{ needs.benchmark-tests.result }}" == "failure" ]] || \
             [[ "${{ needs.vulnerability-testing.result }}" == "failure" ]] || \
             [[ "${{ needs.docker-build.result }}" == "failure" ]]; then
            echo "status=failure" >> $GITHUB_OUTPUT
          else
            echo "status=success" >> $GITHUB_OUTPUT
          fi
      
      - name: Send Slack notification
        if: env.SLACK_WEBHOOK_URL != ''
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
        run: |
          STATUS="${{ steps.status.outputs.status }}"
          COLOR=$([ "$STATUS" = "success" ] && echo "good" || echo "danger")
          EMOJI=$([ "$STATUS" = "success" ] && echo ":white_check_mark:" || echo ":x:")
          
          curl -X POST $SLACK_WEBHOOK_URL \
            -H 'Content-Type: application/json' \
            -d "{
              \"attachments\": [{
                \"color\": \"$COLOR\",
                \"title\": \"$EMOJI Security Testing Pipeline - $STATUS\",
                \"text\": \"Run #${{ github.run_number }} completed\",
                \"fields\": [
                  {\"title\": \"Branch\", \"value\": \"${{ github.ref_name }}\", \"short\": true},
                  {\"title\": \"Commit\", \"value\": \"${{ github.sha }}\", \"short\": true}
                ],
                \"footer\": \"GitHub Actions\",
                \"ts\": $(date +%s)
              }]
            }"
